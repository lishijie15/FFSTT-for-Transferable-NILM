{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write your own Energy Disaggregation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will teach you how to write your own energy disaggregation algorithm, and we will take Seq2Seq_CNN [1] as an example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "- Firstly, before you write your code, you should know that there are three different **mapping** in energy disaggregation algorithms, namely **Seq2Seq**(such as `DAE`, `EnerGAN` and `Seq2Seq_Attention` we implemented), **Seq2Point**(such as `Seq2Point` and `BiLSTM` we implemented) and **Seq2Subseq**(such as `SGN` we implemented).\n",
    "\n",
    "  The math expressions behind them are,\n",
    "\n",
    "   $f_{seq2seq}:Y_{ t-\\lfloor \\frac{W}{2} \\rfloor: t+\\lfloor \\frac{W}{2} \\rfloor} \\rightarrow X_{ t-\\lfloor \\frac{W}{2} \\rfloor: t+\\lfloor \\frac{W}{2} \\rfloor} $\n",
    "\n",
    "  $f_{seq2point}:Y_{ t-\\lfloor \\frac{W}{2} \\rfloor: t+\\lfloor \\frac{W}{2} \\rfloor} \\rightarrow X_{ t} $\n",
    "\n",
    "  $f_{seq2subseq}:Y_{ t-\\lfloor \\frac{W}{2} \\rfloor: t+\\lfloor \\frac{W}{2} \\rfloor} \\rightarrow X_{ t-\\lfloor \\frac{w'}{2} \\rfloor: t+\\lfloor \\frac{w'}{2} \\rfloor}(w'<W) $\n",
    "\n",
    "  Where $Y_{a:b}$ is the **mains reading** from time step a to b and $X_{a:b}$ is the **appliance reading** from time step a to b. Additionally, $t$ represents the **middle point time step** of the given sliding window, and $W$ is the **length** of the given sliding window.\n",
    "\n",
    "  The schematic diagram of them is as follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![BWMpvQ.md.png](https://s1.ax1x.com/2020/11/05/BWMpvQ.md.png)](https://imgchr.com/i/BWMpvQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  When disaggregating mains reading, the sliding windows slide as follow,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![BWMCuj.md.png](https://s1.ax1x.com/2020/11/05/BWMCuj.md.png)](https://imgchr.com/i/BWMCuj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different mappings require different data pipeline, so take care !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Secondly, you should **import packages** we will use and **fix the random seed**, you can easily do this by **copying related code** from other energy disaggregation algorithms such as BiLSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:46:30.267309Z",
     "start_time": "2020-11-05T13:46:28.011494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\think\\Miniconda3\\envs\\mypytorch\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Package import\n",
    "from __future__ import print_function, division\n",
    "from warnings import warn\n",
    "from nilmtk.disaggregate import Disaggregator\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import time\n",
    "\n",
    "# Fix the random seed to ensure the reproducibility of the experiment\n",
    "random_seed = 10\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Use cuda or not\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thirdly, **build your own neural network** as you usually do with Pytorch and describe the **initialization method**.(Refer to **Seq2Point** to know usual **CNN initialization** method and **BiLSTM** for **RNN initialization**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:46:30.278243Z",
     "start_time": "2020-11-05T13:46:30.268261Z"
    }
   },
   "outputs": [],
   "source": [
    "class seq2seqcnn_Pytorch(nn.Module):\n",
    "    def __init__(self, sequence_length):\n",
    "        # Refer to \"ZHANG C, ZHONG M, WANG Z, et al. Sequence-to-point learning with neural networks for non-intrusive load monitoring[C].The 32nd AAAI Conference on Artificial Intelligence\"\n",
    "        super(seq2seqcnn_Pytorch, self).__init__()\n",
    "        self.seq_length = sequence_length\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConstantPad1d((4, 5), 0),\n",
    "            nn.Conv1d(1, 30, 10, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConstantPad1d((3, 4), 0),\n",
    "            nn.Conv1d(30, 30, 8, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConstantPad1d((2, 3), 0),\n",
    "            nn.Conv1d(30, 40, 6, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConstantPad1d((2, 2), 0),\n",
    "            nn.Conv1d(40, 50, 5, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConstantPad1d((2, 2), 0),\n",
    "            nn.Conv1d(50, 50, 5, stride=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(50 * sequence_length, 1024), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, sequence_length)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.dense(x.view(-1, 50 * self.seq_length))\n",
    "        return x.view(-1, sequence_length)\n",
    "    \n",
    "def initialize(layer):\n",
    "    # Xavier_uniform will be applied to conv1d and dense layer, to be sonsistent with Keras and Tensorflow\n",
    "    if isinstance(layer,nn.Conv1d) or isinstance(layer, nn.Linear):    \n",
    "        torch.nn.init.xavier_uniform_(layer.weight.data)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias.data, val = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, write **network training** method. You can **copy** most of the code from other energy disaggregation algorithms, unless you train a **GAN** which requires adversarial training. Notice that **the number of appliance reading dimensions** is different with **different mapping approach** we mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:46:30.296187Z",
     "start_time": "2020-11-05T13:46:30.280231Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(appliance_name,model, mains, appliance, epochs, batch_size, pretrain, checkpoint_interval = None,  train_patience = 3):\n",
    "    # Model configuration\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "    if not pretrain:\n",
    "        model.apply(initialize)\n",
    "    summary(model, (1, mains.shape[1]))\n",
    "    # Split the train and validation set\n",
    "    train_mains,valid_mains,train_appliance,valid_appliance = train_test_split(mains, appliance, test_size=.2, random_state = random_seed)\n",
    "\n",
    "    # Create optimizer, loss function, and dataloader\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    loss_fn = torch.nn.MSELoss(reduction = 'mean')\n",
    "\t# Notice that the number of appliance reading dimensions is different with different mapping approach we mentioned before\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_mains).float().permute(0,2,1), torch.from_numpy(train_appliance).float().permute(0,2,1))\n",
    "    train_loader = tud.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "    valid_dataset = TensorDataset(torch.from_numpy(valid_mains).float().permute(0,2,1), torch.from_numpy(valid_appliance).float().permute(0,2,1))\n",
    "    valid_loader = tud.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "    writer = SummaryWriter(comment = 'train_visual')\n",
    "    patience, best_loss = 0, None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Earlystopping\n",
    "        if(patience == train_patience):\n",
    "            print(\"val_loss did not improve after {} Epochs, thus Earlystopping is calling\".format(train_patience))\n",
    "            break   \n",
    "        # Train the model\n",
    "        st = time.time() \n",
    "        model.train()\n",
    "            \n",
    "        for i, (batch_mains, batch_appliance) in enumerate(train_loader):\n",
    "            if USE_CUDA:\n",
    "                batch_mains = batch_mains.cuda()\n",
    "                batch_appliance = batch_appliance.cuda()\n",
    "            \n",
    "            batch_pred = model(batch_mains)\n",
    "            loss = loss_fn(batch_pred, batch_appliance)\n",
    "\n",
    "            model.zero_grad()    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        ed = time.time()\n",
    "\n",
    "        # Evaluate the model \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            cnt, loss_sum = 0, 0\n",
    "            for i, (batch_mains, batch_appliance) in enumerate(valid_loader):\n",
    "                if USE_CUDA:\n",
    "                    batch_mains = batch_mains.cuda()\n",
    "                    batch_appliance = batch_appliance.cuda()\n",
    "            \n",
    "                batch_pred = model(batch_mains)\n",
    "                loss = loss_fn(batch_appliance, batch_pred)\n",
    "                loss_sum += loss\n",
    "                cnt += 1        \n",
    "\n",
    "        final_loss = loss_sum / cnt\n",
    "        # Save best only\n",
    "        if best_loss is None or final_loss < best_loss:\n",
    "            best_loss = final_loss\n",
    "            patience = 0\n",
    "            net_state_dict = model.state_dict()\n",
    "            path_state_dict = \"./\"+appliance_name+\"_seq2seqcnn_best_state_dict.pt\"\n",
    "            torch.save(net_state_dict, path_state_dict)\n",
    "        else:\n",
    "            patience = patience + 1 \n",
    "\n",
    "        print(\"Epoch: {}, Valid_Loss: {}, Time consumption: {}s.\".format(epoch, final_loss, ed - st))\n",
    "        # For the visualization of training process\n",
    "        for name,param in model.named_parameters():\n",
    "            writer.add_histogram(name + '_grad', param.grad, epoch)\n",
    "            writer.add_histogram(name + '_data', param, epoch)\n",
    "        writer.add_scalars(\"MSELoss\", {\"Valid\":final_loss}, epoch)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (checkpoint_interval != None) and ((epoch + 1) % checkpoint_interval == 0):\n",
    "            checkpoint = {\"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                            \"epoch\": epoch}\n",
    "            path_checkpoint = \"./\"+appliance_name+\"_seq2seqcnn_checkpoint_{}_epoch.pt\".format(epoch)\n",
    "            torch.save(checkpoint, path_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, write **network testing** method. You can **copy** the code from other energy disaggregation algorithms since they **share the same testing method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:46:30.304167Z",
     "start_time": "2020-11-05T13:46:30.297184Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_mains, batch_size = 512):\n",
    "    # Model test\n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    # Create test dataset and dataloader\n",
    "    batch_size = test_mains.shape[0] if batch_size > test_mains.shape[0] else batch_size\n",
    "    test_dataset = TensorDataset(torch.from_numpy(test_mains).float().permute(0,2,1))\n",
    "    test_loader = tud.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
    "    with torch.no_grad():\n",
    "        for i, batch_mains in enumerate(test_loader):\n",
    "            batch_pred = model(batch_mains[0])\n",
    "            if i == 0:\n",
    "                res = batch_pred\n",
    "            else:\n",
    "                res = torch.cat((res, batch_pred), dim = 0)\n",
    "    ed = time.time()\n",
    "    print(\"Inference Time consumption: {}s.\".format(ed - st))\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, complement the Seq2SeqCNN class, there are several functions you should implement. \n",
    "\n",
    "  - `partial_fit`: **Reshape** the training data and **Call for network training** method.\n",
    "  - `disaggregate_chunk`：**Call for network testing** method and **Return predictions**\n",
    "  - `call_preprocessing`: **Pre-process** the training and testing data (**sliding window generation** (implementations are different with Seq2Seq, Seq2Point and Seq2Subseq) and **scale normalization** such as z-score)\n",
    "  - `set_appliance_params`: As the name suggests\n",
    "\n",
    "  Since the relevant code is redundant, so:\n",
    "\n",
    "  if your network is Seq2Seq mapping, you can take use of dae_pytorch.py\n",
    "\n",
    "  if your network is Seq2Point mapping, you can take use of bilstm_pytorch.py\n",
    "\n",
    "  if your network is Seq2Subseq mapping, you can take use of sgn_pytorch.py\n",
    "\n",
    "  In the future, we will write some sealed APIs to simplify and beatify the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:46:30.332130Z",
     "start_time": "2020-11-05T13:46:30.306161Z"
    }
   },
   "outputs": [],
   "source": [
    "class Seq2SeqCNN(Disaggregator):    \n",
    "    def __init__(self, params):\n",
    "        self.MODEL_NAME = \"Seq2SeqCNN\"\n",
    "        self.sequence_length = params.get('sequence_length',129)\n",
    "        self.n_epochs = params.get('n_epochs', 10)\n",
    "        self.batch_size = params.get('batch_size',512)\n",
    "        self.appliance_params = params.get('appliance_params',{})\n",
    "        self.mains_mean = params.get('mains_mean',None)\n",
    "        self.mains_std = params.get('mains_std',None)\n",
    "        self.models = OrderedDict()       \n",
    "        \n",
    "    def partial_fit(self, train_main, train_appliances, pretrain = False, do_preprocessing=True,pretrain_path = \"./seq2seqcnn_pre_state_dict.pkl\",**load_kwargs):         \n",
    "        # If no appliance wise parameters are specified, then they are computed from the data\n",
    "        if len(self.appliance_params) == 0:\n",
    "            self.set_appliance_params(train_appliances)\n",
    "\n",
    "        # Preprocess the data and bring it to a valid shape\n",
    "        if do_preprocessing:\n",
    "            print (\"Doing Preprocessing\")\n",
    "            train_main,train_appliances = self.call_preprocessing(train_main,train_appliances,'train')\n",
    "\n",
    "        train_main = pd.concat(train_main,axis = 0).values\n",
    "        train_main = train_main.reshape((-1,self.sequence_length,1))\n",
    "\n",
    "        new_train_appliances  = []\n",
    "        for app_name, app_df in train_appliances:\n",
    "            app_df = pd.concat(app_df,axis=0).values\n",
    "            app_df = app_df.reshape((-1,self.sequence_length,1))\n",
    "            new_train_appliances.append((app_name, app_df))\n",
    "        train_appliances = new_train_appliances\n",
    "        for appliance_name, power in train_appliances:\n",
    "            if appliance_name not in self.models:\n",
    "                print (\"First model training for \",appliance_name)\n",
    "                self.models[appliance_name] = seq2seqcnn_Pytorch(self.sequence_length)\n",
    "                # Load pretrain dict or not\n",
    "                if pretrain is True:\n",
    "                    self.models[appliance_name].load_state_dict(torch.load(\"./\"+appliance_name+\"_seq2seqcnn_pre_state_dict.pt\"))\n",
    "  \n",
    "            model = self.models[appliance_name]\n",
    "            train(appliance_name, model, train_main, power, self.n_epochs, self.batch_size, pretrain, checkpoint_interval = 3)\n",
    "            # Model test will be based on the best model\n",
    "            self.models[appliance_name].load_state_dict(torch.load(\"./\"+appliance_name+\"_seq2seqcnn_best_state_dict.pt\"))\n",
    "\n",
    "\n",
    "    def disaggregate_chunk(self, test_main_list, do_preprocessing = True):\n",
    "        # Disaggregate (test process)\n",
    "        if do_preprocessing:\n",
    "            test_main_list = self.call_preprocessing(test_main_list,submeters_lst = None,method='test')\n",
    "\n",
    "        test_predictions = []\n",
    "        for test_main in test_main_list:\n",
    "            test_main = test_main.values.reshape((-1,self.sequence_length,1))\n",
    "            disggregation_dict = {}\n",
    "\n",
    "            for appliance in self.models:\n",
    "                # Move the model to cpu, and then test it\n",
    "                model = self.models[appliance].to('cpu')\n",
    "                prediction = test(model, test_main)\n",
    "                app_mean, app_std = self.appliance_params[appliance]['mean'], self.appliance_params[appliance]['std']\n",
    "                prediction = self.denormalize_output(prediction,app_mean,app_std)\n",
    "                valid_predictions = prediction.flatten()\n",
    "                valid_predictions = np.where(valid_predictions > 0, valid_predictions, 0)\n",
    "                series = pd.Series(valid_predictions)\n",
    "                disggregation_dict[appliance] = series\n",
    "            results = pd.DataFrame(disggregation_dict, dtype = 'float32')\n",
    "            test_predictions.append(results)\n",
    "        return test_predictions\n",
    "\n",
    "    def call_preprocessing(self, mains_lst, submeters_lst, method):\n",
    "        # Seq2Seq Version\n",
    "        sequence_length  = self.sequence_length\n",
    "        if method=='train':\n",
    "            # Preprocess the main and appliance data, the parameter 'overlapping' will be set 'True'\n",
    "            processed_mains = []\n",
    "            for mains in mains_lst:\n",
    "                self.mains_mean, self.mains_std = mains.values.mean(), mains.values.std()               \n",
    "                mains = self.normalize_data(mains.values,sequence_length, mains.values.mean(), mains.values.std(),True)\n",
    "                processed_mains.append(pd.DataFrame(mains))\n",
    "\n",
    "            tuples_of_appliances = []\n",
    "            for (appliance_name,app_df_list) in submeters_lst:\n",
    "                app_mean = self.appliance_params[appliance_name]['mean']\n",
    "                app_std = self.appliance_params[appliance_name]['std']\n",
    "                processed_app_dfs = []\n",
    "                for app_df in app_df_list:\n",
    "                    data = self.normalize_data(app_df.values, sequence_length,app_mean,app_std,True)\n",
    "                    processed_app_dfs.append(pd.DataFrame(data))                    \n",
    "                tuples_of_appliances.append((appliance_name, processed_app_dfs))\n",
    "\n",
    "            return processed_mains, tuples_of_appliances\n",
    "\n",
    "        if method=='test':\n",
    "            # Preprocess the main data only, the parameter 'overlapping' will be set 'False'\n",
    "            processed_mains = []\n",
    "            for mains in mains_lst:                \n",
    "                mains = self.normalize_data(mains.values,sequence_length, mains.values.mean(), mains.values.std(),False)\n",
    "                processed_mains.append(pd.DataFrame(mains))\n",
    "            return processed_mains\n",
    "        \n",
    "    def normalize_data(self,data,sequence_length, mean, std, overlapping = False):\n",
    "        # If you want to train the model,then overlapping = True will bring you a lot more training data; else overlapping = false to disaggregate the mains data\n",
    "        n = sequence_length\n",
    "        excess_entries =  sequence_length - (data.size % sequence_length)       \n",
    "        lst = np.array([0] * excess_entries)\n",
    "        arr = np.concatenate((data.flatten(), lst), axis = 0)   \n",
    "        if overlapping:\n",
    "            windowed_x = np.array([ arr[i:i+n] for i in range(len(arr)-n+1) ])\n",
    "        else:\n",
    "            windowed_x = arr.reshape((-1,sequence_length))\n",
    "        # z-score normalization: y = (x - mean)/std\n",
    "        windowed_x = windowed_x - mean\n",
    "        return (windowed_x / std).reshape((-1,sequence_length))\n",
    "\n",
    "    def denormalize_output(self,data,mean,std):\n",
    "        # x = y * std + mean\n",
    "        return mean + data * std\n",
    "    \n",
    "    def set_appliance_params(self,train_appliances):\n",
    "        # Set appliance mean and std to normalize the label(appliance data)\n",
    "        for (app_name, df_list) in train_appliances:\n",
    "            l = np.array(pd.concat(df_list, axis=0))\n",
    "            app_mean = np.mean(l)\n",
    "            app_std = np.std(l)\n",
    "            self.appliance_params.update({app_name:{'mean':app_mean,'std':app_std}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, add your model to `\\nilmtk\\disaggregate\\__init__.py` and enjoy it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to write your own Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take `MCC` as an example,which has already been incorporated in the written framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T13:55:50.251872Z",
     "start_time": "2020-11-05T13:55:50.245923Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, recall_score, precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "on_threhold = {'fridge':50, 'kettle':2000, 'dish washer':10, 'washing machine':20, 'drill':0}\n",
    "\n",
    "def MCC(app_name,app_gt, app_pred):\n",
    "    threshold = on_threhold.get(app_name,10)\n",
    "    gt_temp = np.array(app_gt)\n",
    "    gt_temp = np.where(gt_temp < threshold, 0, 1)\n",
    "    pred_temp = np.array(app_pred)\n",
    "    pred_temp = np.where(pred_temp < threshold,0, 1)\n",
    "\n",
    "    return matthews_corrcoef(gt_temp, pred_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code we can know, there are certain **on-power threholds** for cretain appliances, you can modify them as you need.And then you can write your own metrics by the means of **scikit-learn** which takes (app_name,app_gt, app_pred) as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
